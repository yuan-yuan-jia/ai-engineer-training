{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29d8bf0",
   "metadata": {},
   "source": [
    "## 1. ç®¡ç†å·¥å…·å®‰è£…\n",
    "### 1.1 langchain å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5656e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "\n",
    "# conda \n",
    "# conda install -c conda-forge langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14385d",
   "metadata": {},
   "source": [
    "### 1.2 å…¶ä»–åº“å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›´æ¥è½¬æ¢ä¸º REST API\n",
    "!pip install \"langserve[all]\"\n",
    "\n",
    "# è§‚æµ‹å¹³å°\n",
    "!pip install -U langsmith\n",
    "\n",
    "# LangGraph\n",
    "!pip install -U langgraph\n",
    "\n",
    "# DeepSeek\n",
    "!pip install langchain-deepseek\n",
    "\n",
    "# å¤–éƒ¨èµ„æºé›†æˆ\n",
    "!pip install langchain_community\n",
    "\n",
    "# dotenvç®¡ç†å¯†é’¥ï¼ŒåŠ è½½ç¯å¢ƒå˜é‡ç­‰\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacf945",
   "metadata": {},
   "source": [
    "### 1.3 å¸¸è§çš„å¤„ç†é“¾åŒ…å«ä¸‰ä¸ªè¦ç´ \n",
    "\n",
    "1. è¯­è¨€æ¨¡å‹ï¼šæ ¸å¿ƒæ¨ç†å¼•æ“\n",
    "2. æç¤ºè¯æ¨¡æ¿ï¼šæä¾›æŒ‡ä»¤\n",
    "3. è¾“å‡ºè§£é‡Šå™¨ï¼šè½¬æ¢ä¸ºæ˜“äºä½¿ç”¨çš„æ ¼å¼ï¼Œä¾¿äºä¸‹æ¸¸å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7232d",
   "metadata": {},
   "source": [
    "#### 1.3.1 è¯­è¨€æ¨¡å‹\n",
    "\n",
    "langchainè¯­è¨€æ¨¡å‹ä¸»è¦åˆ†ä¸ºä¸¤ç§ï¼š\n",
    "\n",
    "1. LLM é€šç”¨æ¨¡å‹ï¼šæ¥æ”¶å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå­—ç¬¦ä¸²ä½œä¸ºè¾“å‡ºã€‚\n",
    "2. ChatModel å¯¹è¯æ¨¡å‹ï¼šæ¥æ”¶æ¶ˆæ¯åˆ—è¡¨ï¼Œè¾“å‡ºâ€œæ¶ˆæ¯â€ï¼Œç”¨äºä¸€é—®ä¸€ç­”ã€‚\n",
    "\n",
    "æ¶ˆæ¯ï¼šç”± BaseMessage ç±»å®šä¹‰ï¼ŒåŒ…å«æ–‡æœ¬ã€æ—¶é—´æˆ³ã€ç”¨æˆ·æ ‡è¯†ç¬¦ç­‰å±æ€§ã€‚ä¸»è¦çš„æœ‰\n",
    "\n",
    "1. æ¶ˆæ¯çš„å†…å®¹ï¼šæ–‡æœ¬ï¼Œé€šå¸¸æ˜¯å­—ç¬¦ä¸²\n",
    "2. è§’è‰²ï¼šæ¶ˆæ¯çš„å‘é€æ–¹\n",
    "\n",
    "è§’è‰²ï¼š LangChain ç”¨äºåŒºåˆ†ä¸åŒè§’è‰²çš„å¯¹è±¡\n",
    "\n",
    "1. HumanMessageï¼šäººç±»ï¼ˆç”¨æˆ·ï¼‰è¾“å…¥çš„BaseMessageã€‚\n",
    "2. AIMessageï¼šAIåŠ©æ‰‹ï¼ˆå¤§æ¨¡å‹ï¼‰è¾“å‡ºçš„BaseMessageã€‚\n",
    "3. SystemMessageï¼šç³»ç»Ÿé¢„è®¾çš„BaseMessageã€‚\n",
    "4. FunctionMessageï¼šè‡ªå®šä¹‰å‡½æ•°è¾“å‡ºçš„BaseMessageã€‚\n",
    "5. ToolMessageï¼šè°ƒç”¨ç¬¬ä¸‰æ–¹å·¥å…·è¾“å‡ºçš„BaseMessageã€‚\n",
    "6. ChatMessageï¼šè‡ªå®šä¹‰è§’è‰²ã€‚\n",
    "\n",
    "æ–¹æ³•ï¼š LCEL é»˜è®¤å®ç°åŒæ­¥è°ƒç”¨æ–¹æ³•ï¼Œæœ€å¸¸è§çš„æ˜¯ invoke æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ª BaseMessage å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ª BaseMessage å¯¹è±¡ä½œä¸ºç»“æœã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼š\n",
    "LLMs.invokeï¼šè¾“å…¥è¾“å‡ºéƒ½æ˜¯å­—ç¬¦ä¸²\n",
    "ChatModels.invokeï¼šè¾“å…¥è¾“å‡ºéƒ½æ˜¯BaseMessageå¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6626b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½å‘€ï¼âœ¨ å¾ˆé«˜å…´è§åˆ°ä½ ï¼ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·å‘€ï¼Ÿå¸Œæœ›ä½ åº¦è¿‡äº†æ„‰å¿«çš„ä¸€å¤©ã€‚æˆ‘éšæ—¶å‡†å¤‡å¥½é™ªä½ èŠå¤©ã€å¸®ä½ è§£å†³é—®é¢˜ï¼Œæˆ–è€…å°±è¿™æ ·è½»æ¾æ„‰å¿«åœ°é—²èŠä¸€ä¼šå„¿ã€‚æœ‰ä»€ä¹ˆæƒ³è·Ÿæˆ‘åˆ†äº«çš„å—ï¼Ÿ ğŸŒŸ\n",
      "ä½ å¥½ï¼æˆ‘è¿™é‡Œæ— æ³•ç›´æ¥è·å–ä½ æ‰€åœ¨åœ°çš„å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚ä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ æ‰€åœ¨çš„åŸå¸‚æˆ–åœ°åŒºï¼Œæˆ‘å¯ä»¥å¸®ä½ æä¾›ä¸€äº›æŸ¥è¯¢å¤©æ°”çš„å»ºè®®ï¼Œæˆ–è€…ä½ å¯ä»¥ä½¿ç”¨å¤©æ°”åº”ç”¨æŸ¥çœ‹å®æ—¶å¤©æ°”å“¦ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.tongyi import  Tongyi\n",
    "\n",
    "llm = Tongyi()\n",
    "\n",
    "print(llm.invoke(\"ä½ å¥½\"))\n",
    "print(llm(\"ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24354c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 5, 'total_tokens': 19, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 5}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_08f168e49b_prod0820_fp8_kvcache', 'id': 'b091f984-a197-42cb-94d1-80bfc1bb8631', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--924ee09c-7aac-4ee1-b148-613099d36b53-0' usage_metadata={'input_tokens': 5, 'output_tokens': 14, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "deepseek = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "\n",
    "print(deepseek.invoke(\"ä½ å¥½\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb387ed8",
   "metadata": {},
   "source": [
    "#### 1.3.2 æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "æç¤ºè¯æ¨¡æ¿æ˜¯ç”¨äºç”Ÿæˆæç¤ºè¯çš„æ¨¡æ¿ã€‚ä¾‹å¦‚ï¼Œ`{name}ï¼Œä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æœ‹å‹å°çˆ±åŒå­¦ã€‚`å°±æ˜¯ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ã€‚\n",
    "\n",
    "æç¤ºè¯æ¨¡æ¿ä¸­å¯ä»¥ä½¿ç”¨å˜é‡ï¼Œå˜é‡åç”±å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ç»„æˆï¼Œä¸”å¿…é¡»ä»¥å­—æ¯å¼€å¤´ã€‚ä¾‹å¦‚ï¼Œ`{name}ï¼Œä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æœ‹å‹å°çˆ±åŒå­¦ã€‚`ä¸­çš„`{name}`å°±æ˜¯ä¸€ä¸ªå˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c14018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am a chatbot. How can I help you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Hello, I am a {model_name}. How can I help you today?\"\n",
    ")\n",
    "prompt.format(model_name=\"chatbot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced305f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a helpful AI assistant named Claude.\n",
      "human: Hello, my name is Alice. What's the weather like today?\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ ChatPromptTemplate åˆ›å»ºå¯¹è¯æ¨¡æ¿\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# åˆ›å»ºåŒ…å« system å’Œ human æ¶ˆæ¯çš„èŠå¤©æ¨¡æ¿\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant named {assistant_name}.\"),\n",
    "    (\"human\", \"Hello, my name is {user_name}. {question}\")\n",
    "])\n",
    "\n",
    "# æ ¼å¼åŒ–æ¶ˆæ¯\n",
    "messages = chat_prompt.format_messages(\n",
    "    assistant_name=\"Claude\",\n",
    "    user_name=\"Alice\", \n",
    "    question=\"What's the weather like today?\"\n",
    ")\n",
    "\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01358291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    è¯·ä¸ºåˆå­¦è€…å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ã€‚\n",
      "    å†™ä½œé£æ ¼åº”è¯¥æ˜¯é€šä¿—æ˜“æ‡‚çš„ã€‚\n",
      "\n",
      "    æ–‡ç« è¦æ±‚ï¼š\n",
      "    - å†…å®¹å‡†ç¡®ä¸”æœ‰ç”¨\n",
      "    - ç»“æ„æ¸…æ™°\n",
      "    - é€‚åˆç›®æ ‡å—ä¼—\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# å¤šå˜é‡æ¨¡æ¿\n",
    "complex_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\", \"tone\"],\n",
    "    template=\"\"\"\n",
    "    è¯·ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« ã€‚\n",
    "    å†™ä½œé£æ ¼åº”è¯¥æ˜¯{tone}çš„ã€‚\n",
    "    \n",
    "    æ–‡ç« è¦æ±‚ï¼š\n",
    "    - å†…å®¹å‡†ç¡®ä¸”æœ‰ç”¨\n",
    "    - ç»“æ„æ¸…æ™°\n",
    "    - é€‚åˆç›®æ ‡å—ä¼—\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "formatted_prompt = complex_prompt.format(\n",
    "    topic=\"äººå·¥æ™ºèƒ½\",\n",
    "    audience=\"åˆå­¦è€…\",\n",
    "    tone=\"é€šä¿—æ˜“æ‡‚\"\n",
    ")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94571bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æä»¥ä¸‹é”€å”®æ•°æ®ï¼š\n",
      "Q1é”€å”®é¢: 100ä¸‡, Q2é”€å”®é¢: 120ä¸‡\n",
      "\n",
      "è¯·æä¾›è¶‹åŠ¿åˆ†æã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# f-string é£æ ¼\n",
    "f_string_prompt = PromptTemplate.from_template(\n",
    "    \"åˆ†æä»¥ä¸‹{data_type}æ•°æ®ï¼š\\n{data}\\n\\nè¯·æä¾›{analysis_type}åˆ†æã€‚\"\n",
    ")\n",
    "\n",
    "result = f_string_prompt.format(\n",
    "    data_type=\"é”€å”®\",\n",
    "    data=\"Q1é”€å”®é¢: 100ä¸‡, Q2é”€å”®é¢: 120ä¸‡\",\n",
    "    analysis_type=\"è¶‹åŠ¿\"\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db878476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# å¸¦æ¡ä»¶é€»è¾‘çš„æ¨¡æ¿\n",
    "conditional_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_type\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    {%- if user_type == \"expert\" -%}\n",
    "    ä½œä¸ºä¸“å®¶ï¼Œè¯·è¯¦ç»†å›ç­”ï¼š{question}\n",
    "    {%- else -%}\n",
    "    è¯·ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼å›ç­”ï¼š{question}\n",
    "    {%- endif -%}\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bffdf3",
   "metadata": {},
   "source": [
    "#### 1.3.3 è¾“å‡ºè§£æå™¨\n",
    "\n",
    "è¾“å‡ºè§£æå™¨æ˜¯ LangChain ä¸­çš„é‡è¦ç»„ä»¶ï¼Œç”¨äºå°†å¤§è¯­è¨€æ¨¡å‹(LLM)çš„åŸå§‹æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®æ ¼å¼ï¼Œä¾¿äºç¨‹åºè¿›ä¸€æ­¥å¤„ç†å’Œä½¿ç”¨ã€‚\n",
    "\n",
    "ä¸»è¦åŠŸèƒ½\n",
    "- æ ¼å¼åŒ–è¾“å‡º: å°† LLM çš„è‡ªç„¶è¯­è¨€è¾“å‡ºè§£æä¸º JSONã€XMLã€åˆ—è¡¨ç­‰ç»“æ„åŒ–æ ¼å¼\n",
    "- ç±»å‹è½¬æ¢: å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸º Python å¯¹è±¡ï¼ˆå­—å…¸ã€åˆ—è¡¨ã€è‡ªå®šä¹‰ç±»ç­‰ï¼‰\n",
    "- æ•°æ®éªŒè¯: ç¡®ä¿è¾“å‡ºç¬¦åˆé¢„æœŸçš„æ ¼å¼å’Œçº¦æŸæ¡ä»¶\n",
    "- é”™è¯¯å¤„ç†: å¤„ç†è§£æå¤±è´¥çš„æƒ…å†µï¼Œæä¾›é‡è¯•æœºåˆ¶\n",
    "\n",
    "å¸¸è§çš„è¾“å‡ºè§£æå™¨ç±»å‹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fde359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON è¾“å‡ºè§£æå™¨\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str = Field(description=\"äººç‰©å§“å\")\n",
    "    age: int = Field(description=\"å¹´é¾„\")\n",
    "    occupation: str = Field(description=\"èŒä¸š\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=PersonInfo)\n",
    "\n",
    "# åˆ—è¡¨è¾“å‡ºè§£æå™¨\n",
    "\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "# å°† \"è‹¹æœ,é¦™è•‰,æ©™å­\" è§£æä¸º [\"è‹¹æœ\", \"é¦™è•‰\", \"æ©™å­\"]\n",
    "\n",
    "# ç»“æ„åŒ–è¾“å‡ºè§£æå™¨\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"é—®é¢˜çš„ç­”æ¡ˆ\"),\n",
    "    ResponseSchema(name=\"source\", description=\"ç­”æ¡ˆæ¥æº\")\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ecc570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰‹åŠ¨è§£æç»“æœ: ['Python', 'Java', 'JavaScript', 'C++', 'Go']\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹åŠ¨ä½¿ç”¨è§£æå™¨\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# æ¨¡æ‹Ÿ LLM çš„åŸå§‹è¾“å‡º\n",
    "raw_output = \"Python, Java, JavaScript, C++, Go\"\n",
    "\n",
    "# è§£æè¾“å‡º\n",
    "parsed_result = parser.parse(raw_output)\n",
    "print(\"æ‰‹åŠ¨è§£æç»“æœ:\", parsed_result)\n",
    "# è¾“å‡º: ['Python', 'Java', 'JavaScript', 'C++', 'Go']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c96059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ°´æœåˆ—è¡¨: ['apple', 'banana', 'orange', 'grape', 'strawberry']\n",
      "ç¼–ç¨‹è¯­è¨€åˆ—è¡¨: ['Python', 'Java', 'C++', 'JavaScript', 'Ruby']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "llm = Tongyi(temperature=0)\n",
    "\n",
    "# ç›´æ¥æ„å»ºæç¤ºå¹¶è°ƒç”¨\n",
    "def simple_list_generation(category):\n",
    "    # æ‰‹åŠ¨æ„å»ºæç¤º\n",
    "    prompt = f\"\"\"è¯·åˆ—å‡º5ä¸ª{category}çš„ä¾‹å­ã€‚\n",
    "{parser.get_format_instructions()}\"\"\"\n",
    "    \n",
    "    # ç›´æ¥è°ƒç”¨LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # è§£æç»“æœ\n",
    "    return parser.parse(response)\n",
    "\n",
    "# ä½¿ç”¨\n",
    "fruits = simple_list_generation(\"æ°´æœ\")\n",
    "print(\"æ°´æœåˆ—è¡¨:\", fruits)\n",
    "\n",
    "languages = simple_list_generation(\"ç¼–ç¨‹è¯­è¨€\")\n",
    "print(\"ç¼–ç¨‹è¯­è¨€åˆ—è¡¨:\", languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec799790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¼å¼åŒ–æŒ‡ä»¤: Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "è§£æåçš„ç»“æœ: ['è‹¹æœ', 'é¦™è•‰', 'æ©™å­', 'è‘¡è„', 'è‰è“']\n",
      "ç»“æœç±»å‹: <class 'list'>\n",
      "1. è‹¹æœ\n",
      "2. é¦™è•‰\n",
      "3. æ©™å­\n",
      "4. è‘¡è„\n",
      "5. è‰è“\n",
      "\n",
      "=== ç¼–ç¨‹è¯­è¨€ ===\n",
      "â€¢ Python\n",
      "â€¢ Java\n",
      "â€¢ C++\n",
      "â€¢ JavaScript\n",
      "â€¢ Ruby\n",
      "\n",
      "=== è¿åŠ¨é¡¹ç›® ===\n",
      "â€¢ ç¯®çƒ\n",
      "â€¢ è¶³çƒ\n",
      "â€¢ ç½‘çƒ\n",
      "â€¢ æ¸¸æ³³\n",
      "â€¢ ç”°å¾„\n",
      "\n",
      "=== ä¹å™¨ ===\n",
      "â€¢ é’¢ç´\n",
      "â€¢ å‰ä»–\n",
      "â€¢ å°æç´\n",
      "â€¢ é¼“\n",
      "â€¢ é•¿ç¬›\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "\n",
    "# 1. åˆ›å»ºè¾“å‡ºè§£æå™¨\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 2. è·å–æ ¼å¼åŒ–æŒ‡ä»¤\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(\"æ ¼å¼åŒ–æŒ‡ä»¤:\", format_instructions)\n",
    "\n",
    "# 3. åˆ›å»ºåŒ…å«æ ¼å¼æŒ‡ä»¤çš„æç¤ºæ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    template=\"è¯·åˆ—å‡º5ä¸ª{category}çš„ä¾‹å­ã€‚\\n{format_instructions}\",\n",
    "    input_variables=[\"category\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# 4. åˆå§‹åŒ– LLM\n",
    "llm = Tongyi(temperature=0)\n",
    "\n",
    "# 5. åˆ›å»ºå®Œæ•´çš„é“¾\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 6. ä½¿ç”¨ç¤ºä¾‹\n",
    "try:\n",
    "    # è°ƒç”¨é“¾å¹¶è·å–è§£æåçš„ç»“æœ\n",
    "    result = chain.invoke({\"category\": \"æ°´æœ\"})\n",
    "    print(\"è§£æåçš„ç»“æœ:\", result)\n",
    "    print(\"ç»“æœç±»å‹:\", type(result))\n",
    "    # è¾“å‡ºç¤ºä¾‹: ['è‹¹æœ', 'é¦™è•‰', 'æ©™å­', 'è‘¡è„', 'è‰è“']\n",
    "    \n",
    "    # éå†ç»“æœ\n",
    "    for i, fruit in enumerate(result, 1):\n",
    "        print(f\"{i}. {fruit}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"è§£æé”™è¯¯: {e}\")\n",
    "\n",
    "# 7. æ›´å¤æ‚çš„ç¤ºä¾‹ - å¤šä¸ªç±»åˆ«\n",
    "categories = [\"ç¼–ç¨‹è¯­è¨€\", \"è¿åŠ¨é¡¹ç›®\", \"ä¹å™¨\"]\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== {category} ===\")\n",
    "    try:\n",
    "        result = chain.invoke({\"category\": category})\n",
    "        for item in result:\n",
    "            print(f\"â€¢ {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç† {category} æ—¶å‡ºé”™: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7bafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©é¤è´­ç‰©æ¸…å•: ['ç‰›å¥¶', 'é¸¡è›‹', 'é¢åŒ…', 'é»„æ²¹', 'ç‡•éº¦ç‰‡']\n",
      "åˆé¤è´­ç‰©æ¸…å•: ['é¢åŒ…', 'ç«è…¿', 'ç”Ÿèœ', 'ç•ªèŒ„', 'å¥¶é…ª']\n",
      "æ™šé¤è´­ç‰©æ¸…å•: ['é¸¡èƒ¸è‚‰', 'è¥¿å…°èŠ±', 'çº¢æ¤’', 'å¤§è’œ', 'æ©„æ¦„æ²¹']\n",
      "\n",
      "=== è¯¦ç»†è´­ç‰©æ¸…å• ===\n",
      "\n",
      "ğŸ½ï¸ æ—©é¤:\n",
      "  1. ç‰›å¥¶\n",
      "  2. é¸¡è›‹\n",
      "  3. é¢åŒ…\n",
      "  4. é»„æ²¹\n",
      "  5. ç‡•éº¦ç‰‡\n",
      "\n",
      "ğŸ½ï¸ åˆé¤:\n",
      "  1. é¢åŒ…\n",
      "  2. ç«è…¿\n",
      "  3. ç”Ÿèœ\n",
      "  4. ç•ªèŒ„\n",
      "  5. å¥¶é…ª\n",
      "\n",
      "ğŸ½ï¸ æ™šé¤:\n",
      "  1. é¸¡èƒ¸è‚‰\n",
      "  2. è¥¿å…°èŠ±\n",
      "  3. çº¢æ¤’\n",
      "  4. å¤§è’œ\n",
      "  5. æ©„æ¦„æ²¹\n"
     ]
    }
   ],
   "source": [
    "# è´­ç‰©æ¸…å•\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 1. åˆå§‹åŒ–ç»„ä»¶\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "llm = Tongyi(temperature=0)\n",
    "\n",
    "# 2. åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "shopping_prompt = PromptTemplate(\n",
    "    template=\"æ ¹æ®{meal_type}ï¼Œç”Ÿæˆä¸€ä¸ªåŒ…å«5ä¸ªé£Ÿæçš„è´­ç‰©æ¸…å•ã€‚\\n{format_instructions}\",\n",
    "    input_variables=[\"meal_type\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 3. åˆ›å»ºé“¾\n",
    "shopping_chain = shopping_prompt | llm | parser\n",
    "\n",
    "# 4. ç”Ÿæˆä¸åŒé¤å‹çš„è´­ç‰©æ¸…å•\n",
    "meals = [\"æ—©é¤\", \"åˆé¤\", \"æ™šé¤\"]\n",
    "shopping_lists = {}\n",
    "\n",
    "for meal in meals:\n",
    "    shopping_lists[meal] = shopping_chain.invoke({\"meal_type\": meal})\n",
    "    print(f\"{meal}è´­ç‰©æ¸…å•: {shopping_lists[meal]}\")\n",
    "\n",
    "# 5. æ ¼å¼åŒ–è¾“å‡º\n",
    "print(\"\\n=== è¯¦ç»†è´­ç‰©æ¸…å• ===\")\n",
    "for meal, items in shopping_lists.items():\n",
    "    print(f\"\\nğŸ½ï¸ {meal}:\")\n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"  {i}. {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©é¤é£Ÿæ: ['é¸¡è›‹', 'ç‰›å¥¶', 'é¢åŒ…', 'ç‡•éº¦', 'é¦™è•‰']\n",
      "åŠå…¬å®¤ç”¨å“: ['Pen', 'Notebook', 'Stapler', 'Printer', 'Coffee Maker']\n",
      "æ—…è¡Œå¿…éœ€å“: ['æŠ¤ç…§', 'é’±åŒ…', 'æ‰‹æœºå……ç”µå™¨', 'æ°´æ¯', 'é˜²æ™’éœœ']\n",
      "å¥èº«å™¨æ: ['å“‘é“ƒ', 'æ é“ƒ', 'å¼•ä½“å‘ä¸Šæ†', 'å¥èº«çƒ', 'è·³ç»³']\n",
      "å­¦ä¹ å·¥å…·: ['Anki', 'Quizlet', 'Notion', 'Duolingo', 'Khan Academy']\n"
     ]
    }
   ],
   "source": [
    "# å¤šåœºæ™¯\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# é€šç”¨åˆ—è¡¨ç”Ÿæˆé“¾\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "list_chain = (\n",
    "    PromptTemplate(\n",
    "        template=\"è¯·åˆ—å‡º5ä¸ª{category}çš„{item_type}ã€‚\\n{format_instructions}\",\n",
    "        input_variables=[\"category\", \"item_type\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "    | Tongyi(temperature=0)\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# å¤šåœºæ™¯ä½¿ç”¨\n",
    "scenarios = [\n",
    "    {\"category\": \"æ—©é¤\", \"item_type\": \"é£Ÿæ\"},\n",
    "    {\"category\": \"åŠå…¬å®¤\", \"item_type\": \"ç”¨å“\"},\n",
    "    {\"category\": \"æ—…è¡Œ\", \"item_type\": \"å¿…éœ€å“\"},\n",
    "    {\"category\": \"å¥èº«\", \"item_type\": \"å™¨æ\"},\n",
    "    {\"category\": \"å­¦ä¹ \", \"item_type\": \"å·¥å…·\"}\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    result = list_chain.invoke(scenario)\n",
    "    print(f\"{scenario['category']}{scenario['item_type']}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f802b31",
   "metadata": {},
   "source": [
    "#### 1.3.4 ä½¿ç”¨ LCEL çš„å¥½å¤„\n",
    "\n",
    "1. ç®€æ´è¯­æ³•: ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ | åˆ›å»ºé“¾ï¼Œä»£ç ç›´è§‚æ˜“è¯»\n",
    "2. è‡ªåŠ¨ç±»å‹æ¨æ–­: ç»„ä»¶é—´æ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢ï¼Œå‡å°‘æ‰‹åŠ¨å¤„ç†\n",
    "3. å¹¶è¡Œå¤„ç†: å†…ç½®å¹¶è¡Œæ‰§è¡Œèƒ½åŠ›ï¼Œæé«˜å¤„ç†æ•ˆç‡\n",
    "4. æµå¼å¤„ç†: åŸç”Ÿæ”¯æŒæµå¼è¾“å‡ºï¼Œé€‚åˆå®æ—¶åº”ç”¨\n",
    "5. æ™ºèƒ½ç¼“å­˜: è‡ªåŠ¨ç¼“å­˜ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—\n",
    "6. å†…ç½®è°ƒè¯•: æ›´å¥½çš„è°ƒè¯•å’Œç›‘æ§èƒ½åŠ›\n",
    "7. æ’ä»¶åŒ–: æ”¯æŒè‡ªå®šä¹‰ç»„ä»¶å’Œæ‰©å±•\n",
    "8. æ ‡å‡†æ¥å£: ç»Ÿä¸€çš„ Runnable æ¥å£\n",
    "9. ç‰ˆæœ¬ç®¡ç†: æ”¯æŒé“¾çš„ç‰ˆæœ¬æ§åˆ¶å’Œç®¡ç†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
